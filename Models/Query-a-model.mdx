---

title: 'Query a model'

description: 'Make inference requests on your models.'

---

Model deployments on Beamlit have an **inference endpoint** which can be used by external consumers to request an inference execution. Inference requests are then routed on Global Inference Network based on the deployment policies associated with your model deployment. 

## Inference endpoints

Whenever you deploy a model on Beamlit, an **inference endpoint** is generated on Global Inference Network.

The inference URL looks like this:

`edge-gw.beamlit.net/your-workspace/your-model`

There is one distinct endpoint for each **model deployment,** i.e. for each combination of a model and an environment on which it is deployed. 

For example, if you have one version of model “your-model” deployed on the *production* environment and one version deployed on the *development* environment:

- `edge-gw.beamlit.net/your-workspace/your-model?environment=production` will call the production deployment
- `edge-gw.beamlit.net/your-workspace/your-model?environment=development` will call the environment deployment

If you do not specify the environment in the inference request, it will call the *production* environment by default. If the model is not deployed on the production environment, it will return an error.

### Specific API endpoints in your model

The URL above hosts your model and can be called directly in most cases. However your model may **implement additional endpoints.** These sub-endpoints will be hosted on this URL.

For example, if you deploy a text generation model that also implements the ChatCompletions API:

- calling `edge-gw.beamlit.net/workspace/model` (the base endpoint) will generate text based on a prompt
- calling `edge-gw.beamlit.net/workspace/model/v1/chat/completions` (the ChatCompletions API implementation) will generate response based on a list of messages

### Endpoint authentication

By default, models deployed on Beamlit aren’t public. It is necessary to authenticate all inference requests, via a bearer token.

The evaluation of authentication/authorization for inference requests is managed by the Global Inference Network based on the access given in your workspace:

- Admins can query all models deployed in the workspace
- Simple members can only query the models that they have been given access to

> Making a model publicly available is not yet available. Please contact us if this is something that you need today.

## Make an inference request

### Beamlit API

Call the inference endpoint for the model deployment you are requesting:

< TO COMPLETE >

Read about the API parameters in the reference.

### Beamlit CLI

Make an inference request to the model deployment by running the following command:

< TO COMPLETE >

Read about the CLI parameters in the reference.

### Beamlit console

Inference requests can be made from the Beamlit console from the model deployment’s workbench page.

[screen of workbench from user interface]